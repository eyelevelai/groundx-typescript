/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Configurations for the LLM an agent uses in a workflow
 */
export interface WorkflowEngine {
    /** A token that is added to the header of a request as an authorization bearer token */
    apiKey?: string;
    /** The base URL that precedes '/chat/completion' for an OpenAI chat completion-compatible endpoint */
    baseURL?: string;
    /** The model name that will be included in the request */
    engineID?: string;
    /** An enumerated value that conforms to OpenAI '/chat/completion' specifications */
    reasoningEffort?: WorkflowEngine.ReasoningEffort;
    /** An enumerated descriptor of the service type, impacts how the requests are configured */
    service?: WorkflowEngine.Service;
}

export namespace WorkflowEngine {
    /**
     * An enumerated value that conforms to OpenAI '/chat/completion' specifications
     */
    export type ReasoningEffort = "minimal" | "low" | "medium" | "high";
    export const ReasoningEffort = {
        Minimal: "minimal",
        Low: "low",
        Medium: "medium",
        High: "high",
    } as const;
    /**
     * An enumerated descriptor of the service type, impacts how the requests are configured
     */
    export type Service = "openai" | "openai-base64" | "azure" | "deep-infra" | "hosted";
    export const Service = {
        Openai: "openai",
        OpenaiBase64: "openai-base64",
        Azure: "azure",
        DeepInfra: "deep-infra",
        Hosted: "hosted",
    } as const;
}
